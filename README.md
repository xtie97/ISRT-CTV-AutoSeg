# Multi-Modality Artificial Intelligence for Involved-Site Radiation Therapy: Clinical Target Volume Delineation in High-Risk Pediatric Hodgkin Lymphoma

## ðŸ“Œ Overview

This repository holds the code for the Red Journal paper ***Multi-Modality Artificial Intelligence for Involved-Site Radiation Therapy: Clinical Target Volume Delineation in High-Risk Pediatric Hodgkin Lymphoma***. We aim to develop automated CTV segmentation algorithms that integrated multi-modality imaging to facilitate ISRT planning.

This study included planning CT, baseline PET/CT (PET1), and interim PET/CT (PET2) scans from 288 pediatric patients with high-risk HL enrolled in the COG AHOD 1331 trial. Data from 58 patients across 24 institutions were held out for external testing, while the remaining 230 cases from 95 institutions were used for model development. We investigated three deep learning (DL) architectures (SegResNet, ResUNet, and SwinUNETR) and evaluated the impact of incorporating PET1 and PET2 images along with the planning CT. Performance was assessed using the Dice similarity coefficient (DSC) and 95% Hausdorff distance (HD95). Inter-observer variability (IOV) was estimated by comparing original institutional CTVs with those newly delineated by four radiation oncologists on 10 cases. The quality of CTVs generated by the top-performing model was independently assessed by radiation oncologists on 40 other cases using a 5-point Likert scale and compared against the original institutional CTVs.

On the external cohort, a SwinUNETR model incorporating planning CT, PET1, and PET2 images achieved the highest performance, with a DSC of 0.72 and HD95 of 34.43 mm. All models incorporating PET/CT images were significantly better (P<0.01) than planning CT-only models. IOV analysis yielded a DSC of 0.70 and HD95 of 30.14 mm. In clinical evaluation, DL-generated CTVs received a mean quality score of 3.38 out of 5, comparable to original physician-delineated CTVs (3.13; Pâ€¯=â€¯0.13)

The DL model was able to generate clinically useful CTVs with quality comparable to manually delineated CTVs, suggesting its potential to improve physician efficiency in ISRT planning.
---

## Key Features

- **First deep learning framework** developed specifically for automated ISRT CTV delineation in **pediatric Hodgkin lymphoma**

- **Longitudinal multi-modality imaging integration**, incorporating:
  - Planning CT  
  - Baseline PET/CT (PET1)  
  - Interim PET/CT (PET2)

- **Comparison of model designs**, including:
  - CNN-based vs. Transformer-based architectures  
  - Early fusion vs. late fusion strategies for multi-modality integration

- **External validation** on **58 patients from 24 institutions**

- **Inter-observer variability (IOV) benchmarking** against contours generated by board-certified radiation oncologists

- **Blinded clinical reader study** demonstrating that DL-generated CTVs achieve **contour quality comparable to physician-delineated CTVs**

---

ðŸ—‚ Repository Structure
```bash
ISRT-CTV-AutoSeg/
â”œâ”€â”€ CT_only/                # Code for CT only models (with only planning CT as input)
â”œâ”€â”€ Early_fusion/           # Code for early fusion models (either baseline PET/CT+interim PET/CT+Planning CT or baseline PET/CT+Planning CT)
â”œâ”€â”€ Late_fusion/                 # Code for late fusion mdoels
â”œâ”€â”€ Compute_metrics.py      # Code for computing segmentation metrics (DICE, HD95, ASSD)
â””â”€â”€ README.md
```

âš™ï¸ Installation
1. Clone the Repository
```bash
git clone https://github.com/xtie97/ISRT-CTV-AutoSeg.git
cd ISRT-CTV-AutoSeg
```

2. Docker imaging
```bash
conda create -n isrt_ctv python=3.8 -y
conda activate isrt_ctv
```

ðŸš€ Training
```bash
python training/train.py \
  --config configs/train_swinunetr_late_fusion.yaml
```

ðŸ” Inference
```bash
python inference/run_inference.py \
  --config configs/infer.yaml \
  --checkpoint path/to/model.pth
```

Sliding-window inference

Gaussian blending

Ensemble averaging

Output: binary CTV mask (NIfTI)
